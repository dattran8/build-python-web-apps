{"cells":[{"cell_type":"markdown","id":"bbf7941b-6c0a-4b14-b1fc-c703e57e352b","metadata":{},"source":["# Building Multimodal AI Applications with LangChain & the OpenAI API "]},{"cell_type":"markdown","id":"333b6c1a-a3c4-4c3c-b5e1-145bd214f4e0","metadata":{},"source":["## Goals "]},{"cell_type":"markdown","id":"701b76fe-04db-405c-98f7-f0f5babd84b4","metadata":{},"source":["Videos can be full of useful information, but getting hold of that info can be slow, since you need to watch the whole thing or try skipping through it. It can be much faster to use a bot to ask questions about the contents of the transcript.\n","\n","In this project, you'll download a tutorial video from YouTube, transcribe the audio, and create a simple Q&A bot to ask questions about the content."]},{"cell_type":"markdown","id":"3e302e1c-4c18-4c44-87fd-ba935c3a0853","metadata":{},"source":["- Understanding the building blocks of working with Multimodal AI projects\n","- Working with some of the fundamental concepts of LangChain  \n","- How to use the Whisper API to transcribe audio to text \n","- How to combine both LangChain and Whisper API to create ask questions of any YouTube video "]},{"cell_type":"markdown","id":"8231d2c6-275e-4399-b7cd-84e112831d08","metadata":{},"source":["## Before you begin"]},{"cell_type":"markdown","id":"785d7fac-edb2-482f-be2b-c63dc2882103","metadata":{},"source":["You'll need a developer account with [OpenAI ](https://auth0.openai.com/u/signup/identifier?state=hKFo2SAyeTZBU1pzbUNWYWs3Wml5OWVvUVh4enZldC1LYU9PMaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIDFUakNoUGFMLUdNWFpfQkpqdncyZjVDQk9xUTE4U0xDo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q) and a create API Key. The API secret key will be stored in your 'Environment Variables' on the side menu. See the *getting-started.ipynb* notebook for details on setting this up."]},{"cell_type":"markdown","id":"a9274661-8d8c-4cc5-901e-5fc497866b89","metadata":{},"source":["## Task 0: Setup"]},{"cell_type":"markdown","id":"823598ac-fa77-4532-997d-2923d0017e90","metadata":{},"source":["The project requires several packages that need to be installed into Workspace.\n","\n","- `langchain` is a framework for developing generative AI applications.\n","- `yt_dlp` lets you download YouTube videos.\n","- `tiktoken` converts text into tokens.\n","- `docarray` makes it easier to work with multi-model data (in this case mixing audio and text)."]},{"cell_type":"markdown","id":"c17ab340-c582-4ba7-ab33-5d582210f5c2","metadata":{},"source":["### Instructions\n","\n","Run the following code to install the packages."]},{"cell_type":"code","execution_count":1,"id":"e1ca41e3-2dfd-4b9a-b595-e5af720ca36a","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":10465,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1694705467366,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install langchain yt_dlp","outputsMetadata":{"0":{"height":463,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting langchain==0.0.292\n","  Using cached langchain-0.0.292-py3-none-any.whl.metadata (14 kB)\n","Collecting PyYAML>=5.3 (from langchain==0.0.292)\n","  Using cached PyYAML-6.0.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (2.1 kB)\n","Collecting SQLAlchemy<3,>=1.4 (from langchain==0.0.292)\n","  Using cached SQLAlchemy-2.0.29-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (9.6 kB)\n","Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.0.292)\n","  Using cached aiohttp-3.9.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (7.5 kB)\n","Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.292)\n","  Using cached dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n","Collecting langsmith<0.1.0,>=0.0.21 (from langchain==0.0.292)\n","  Using cached langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n","Collecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.292)\n","  Using cached numexpr-2.10.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (7.9 kB)\n","Collecting numpy<2,>=1 (from langchain==0.0.292)\n","  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (62 kB)\n","Collecting pydantic<3,>=1 (from langchain==0.0.292)\n","  Using cached pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n","Collecting requests<3,>=2 (from langchain==0.0.292)\n","  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n","Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.292)\n","  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n","Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.292)\n","  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n","Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.292)\n","  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n","Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.292)\n","  Using cached frozenlist-1.4.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (12 kB)\n","Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.292)\n","  Using cached multidict-6.0.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.2 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.292)\n","  Using cached yarl-1.9.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (31 kB)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.292)\n","  Using cached marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.292)\n","  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain==0.0.292)\n","  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n","Collecting pydantic-core==2.18.2 (from pydantic<3,>=1->langchain==0.0.292)\n","  Using cached pydantic_core-2.18.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.5 kB)\n","Collecting typing-extensions>=4.6.1 (from pydantic<3,>=1->langchain==0.0.292)\n","  Using cached typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n","Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain==0.0.292)\n","  Using cached charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (33 kB)\n","Collecting idna<4,>=2.5 (from requests<3,>=2->langchain==0.0.292)\n","  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n","Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain==0.0.292)\n","  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n","Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain==0.0.292)\n","  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n","Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.0.292)\n","  Using cached greenlet-3.0.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.8 kB)\n","Requirement already satisfied: packaging>=17.0 in /home/vscode/.local/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.292) (24.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.292)\n","  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Using cached langchain-0.0.292-py3-none-any.whl (1.7 MB)\n","Using cached aiohttp-3.9.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.3 MB)\n","Using cached dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n","Using cached langsmith-0.0.92-py3-none-any.whl (56 kB)\n","Using cached numexpr-2.10.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (383 kB)\n","Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (13.9 MB)\n","Using cached pydantic-2.7.1-py3-none-any.whl (409 kB)\n","Using cached pydantic_core-2.18.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.8 MB)\n","Using cached PyYAML-6.0.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (696 kB)\n","Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n","Using cached SQLAlchemy-2.0.29-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.2 MB)\n","Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n","Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n","Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n","Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n","Using cached charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (137 kB)\n","Using cached frozenlist-1.4.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (280 kB)\n","Using cached greenlet-3.0.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (660 kB)\n","Using cached idna-3.7-py3-none-any.whl (66 kB)\n","Using cached marshmallow-3.21.1-py3-none-any.whl (49 kB)\n","Using cached multidict-6.0.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (129 kB)\n","Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n","Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n","Using cached yarl-1.9.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (317 kB)\n","Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: urllib3, typing-extensions, tenacity, PyYAML, numpy, mypy-extensions, multidict, marshmallow, idna, greenlet, frozenlist, charset-normalizer, certifi, attrs, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, pydantic-core, numexpr, aiosignal, pydantic, dataclasses-json, aiohttp, langsmith, langchain\n","Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.29 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.6.0 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 dataclasses-json-0.5.14 frozenlist-1.4.1 greenlet-3.0.3 idna-3.7 langchain-0.0.292 langsmith-0.0.92 marshmallow-3.21.1 multidict-6.0.5 mypy-extensions-1.0.0 numexpr-2.10.0 numpy-1.26.4 pydantic-2.7.1 pydantic-core-2.18.2 requests-2.31.0 tenacity-8.2.3 typing-extensions-4.11.0 typing-inspect-0.9.0 urllib3-2.2.1 yarl-1.9.4\n"]}],"source":["# Install langchain\n","!pip install langchain==0.0.292"]},{"cell_type":"code","execution_count":2,"id":"4eb141d6-8eaf-4fe2-b29a-e58765a02252","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":8477,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1694802259584,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Install yt_dlp\n!pip install yt_dlp==2023.7.6","outputsMetadata":{"0":{"height":447,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting yt_dlp==2023.7.6\n","  Using cached yt_dlp-2023.7.6-py2.py3-none-any.whl.metadata (157 kB)\n","Collecting mutagen (from yt_dlp==2023.7.6)\n","  Using cached mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n","Collecting pycryptodomex (from yt_dlp==2023.7.6)\n","  Using cached pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.4 kB)\n","Collecting websockets (from yt_dlp==2023.7.6)\n","  Using cached websockets-12.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.6 kB)\n","Requirement already satisfied: certifi in /home/vscode/.local/lib/python3.12/site-packages (from yt_dlp==2023.7.6) (2024.2.2)\n","Collecting brotli (from yt_dlp==2023.7.6)\n","  Using cached Brotli-1.1.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.5 kB)\n","Using cached yt_dlp-2023.7.6-py2.py3-none-any.whl (3.0 MB)\n","Using cached Brotli-1.1.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.9 MB)\n","Using cached mutagen-1.47.0-py3-none-any.whl (194 kB)\n","Using cached pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.1 MB)\n","Using cached websockets-12.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (131 kB)\n","Installing collected packages: brotli, websockets, pycryptodomex, mutagen, yt_dlp\n","Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.20.0 websockets-12.0 yt_dlp-2023.7.6\n"]}],"source":["# Install yt_dlp\n","!pip install yt_dlp==2023.7.6"]},{"cell_type":"code","execution_count":3,"id":"3dce7fdd-9f8a-4517-a1bb-d03221dddc9e","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":6113,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1694705815654,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install tiktoken docarray","outputsMetadata":{"0":{"height":447,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting tiktoken==0.5.1\n","  Using cached tiktoken-0.5.1.tar.gz (32 kB)\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken==0.5.1)\n","  Using cached regex-2024.4.16-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (40 kB)\n","Requirement already satisfied: requests>=2.26.0 in /home/vscode/.local/lib/python3.12/site-packages (from tiktoken==0.5.1) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/vscode/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.1) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.1) (2024.2.2)\n","Using cached regex-2024.4.16-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (785 kB)\n","Building wheels for collected packages: tiktoken\n","  Building wheel for tiktoken (pyproject.toml) ... \u001b[?25lerror\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tiktoken \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m \u001b[31m[38 lines of output]\u001b[0m\n","  \u001b[31m   \u001b[0m running bdist_wheel\n","  \u001b[31m   \u001b[0m running build\n","  \u001b[31m   \u001b[0m running build_py\n","  \u001b[31m   \u001b[0m creating build\n","  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-cpython-312\n","  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-cpython-312/tiktoken\n","  \u001b[31m   \u001b[0m copying tiktoken/model.py -> build/lib.linux-aarch64-cpython-312/tiktoken\n","  \u001b[31m   \u001b[0m copying tiktoken/_educational.py -> build/lib.linux-aarch64-cpython-312/tiktoken\n","  \u001b[31m   \u001b[0m copying tiktoken/registry.py -> build/lib.linux-aarch64-cpython-312/tiktoken\n","  \u001b[31m   \u001b[0m copying tiktoken/core.py -> build/lib.linux-aarch64-cpython-312/tiktoken\n","  \u001b[31m   \u001b[0m copying tiktoken/__init__.py -> build/lib.linux-aarch64-cpython-312/tiktoken\n","  \u001b[31m   \u001b[0m copying tiktoken/load.py -> build/lib.linux-aarch64-cpython-312/tiktoken\n","  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-cpython-312/tiktoken_ext\n","  \u001b[31m   \u001b[0m copying tiktoken_ext/openai_public.py -> build/lib.linux-aarch64-cpython-312/tiktoken_ext\n","  \u001b[31m   \u001b[0m running egg_info\n","  \u001b[31m   \u001b[0m writing tiktoken.egg-info/PKG-INFO\n","  \u001b[31m   \u001b[0m writing dependency_links to tiktoken.egg-info/dependency_links.txt\n","  \u001b[31m   \u001b[0m writing requirements to tiktoken.egg-info/requires.txt\n","  \u001b[31m   \u001b[0m writing top-level names to tiktoken.egg-info/top_level.txt\n","  \u001b[31m   \u001b[0m reading manifest file 'tiktoken.egg-info/SOURCES.txt'\n","  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n","  \u001b[31m   \u001b[0m warning: no files found matching 'Makefile'\n","  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n","  \u001b[31m   \u001b[0m writing manifest file 'tiktoken.egg-info/SOURCES.txt'\n","  \u001b[31m   \u001b[0m copying tiktoken/py.typed -> build/lib.linux-aarch64-cpython-312/tiktoken\n","  \u001b[31m   \u001b[0m running build_ext\n","  \u001b[31m   \u001b[0m running build_rust\n","  \u001b[31m   \u001b[0m error: can't find Rust compiler\n","  \u001b[31m   \u001b[0m \n","  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n","  \u001b[31m   \u001b[0m \n","  \u001b[31m   \u001b[0m To update pip, run:\n","  \u001b[31m   \u001b[0m \n","  \u001b[31m   \u001b[0m     pip install --upgrade pip\n","  \u001b[31m   \u001b[0m \n","  \u001b[31m   \u001b[0m and then retry package installation.\n","  \u001b[31m   \u001b[0m \n","  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n","  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","\u001b[?25h\u001b[31m  ERROR: Failed building wheel for tiktoken\u001b[0m\u001b[31m\n","\u001b[0mFailed to build tiktoken\n","\u001b[31mERROR: Could not build wheels for tiktoken, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install tiktoken==0.5.1"]},{"cell_type":"code","execution_count":4,"id":"27c31b16","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting tiktoken\n","  Using cached tiktoken-0.6.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.6 kB)\n","Collecting regex>=2022.1.18 (from tiktoken)\n","  Using cached regex-2024.4.16-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (40 kB)\n","Requirement already satisfied: requests>=2.26.0 in /home/vscode/.local/lib/python3.12/site-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/vscode/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n","Using cached tiktoken-0.6.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.8 MB)\n","Using cached regex-2024.4.16-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (785 kB)\n","Installing collected packages: regex, tiktoken\n","Successfully installed regex-2024.4.16 tiktoken-0.6.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install --upgrade tiktoken"]},{"cell_type":"code","execution_count":5,"id":"6a7d041f-ea6c-470f-a3ca-1361f161021d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting docarray==0.38.0\n","  Using cached docarray-0.38.0-py3-none-any.whl.metadata (36 kB)\n","Requirement already satisfied: numpy>=1.17.3 in /home/vscode/.local/lib/python3.12/site-packages (from docarray==0.38.0) (1.26.4)\n","Collecting orjson>=3.8.2 (from docarray==0.38.0)\n","  Using cached orjson-3.10.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (49 kB)\n","Collecting pydantic<2.0.0,>=1.10.2 (from docarray==0.38.0)\n","  Using cached pydantic-1.10.15-py3-none-any.whl.metadata (150 kB)\n","Collecting rich>=13.1.0 (from docarray==0.38.0)\n","  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n","Collecting types-requests>=2.28.11.6 (from docarray==0.38.0)\n","  Using cached types_requests-2.31.0.20240406-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: typing-inspect>=0.8.0 in /home/vscode/.local/lib/python3.12/site-packages (from docarray==0.38.0) (0.9.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /home/vscode/.local/lib/python3.12/site-packages (from pydantic<2.0.0,>=1.10.2->docarray==0.38.0) (4.11.0)\n","Collecting markdown-it-py>=2.2.0 (from rich>=13.1.0->docarray==0.38.0)\n","  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vscode/.local/lib/python3.12/site-packages (from rich>=13.1.0->docarray==0.38.0) (2.17.2)\n","Requirement already satisfied: urllib3>=2 in /home/vscode/.local/lib/python3.12/site-packages (from types-requests>=2.28.11.6->docarray==0.38.0) (2.2.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /home/vscode/.local/lib/python3.12/site-packages (from typing-inspect>=0.8.0->docarray==0.38.0) (1.0.0)\n","Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray==0.38.0)\n","  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n","Using cached docarray-0.38.0-py3-none-any.whl (264 kB)\n","Using cached orjson-3.10.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (147 kB)\n","Using cached pydantic-1.10.15-py3-none-any.whl (159 kB)\n","Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n","Using cached types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n","Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n","Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n","Installing collected packages: types-requests, pydantic, orjson, mdurl, markdown-it-py, rich, docarray\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 2.7.1\n","    Uninstalling pydantic-2.7.1:\n","      Successfully uninstalled pydantic-2.7.1\n","Successfully installed docarray-0.38.0 markdown-it-py-3.0.0 mdurl-0.1.2 orjson-3.10.1 pydantic-1.10.15 rich-13.7.1 types-requests-2.31.0.20240406\n"]}],"source":["!pip install docarray==0.38.0"]},{"cell_type":"code","execution_count":6,"id":"1c51e703","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: docarray in /home/vscode/.local/lib/python3.12/site-packages (0.38.0)\n","Collecting docarray\n","  Using cached docarray-0.40.0-py3-none-any.whl.metadata (36 kB)\n","Requirement already satisfied: numpy>=1.17.3 in /home/vscode/.local/lib/python3.12/site-packages (from docarray) (1.26.4)\n","Requirement already satisfied: orjson>=3.8.2 in /home/vscode/.local/lib/python3.12/site-packages (from docarray) (3.10.1)\n","Requirement already satisfied: pydantic>=1.10.8 in /home/vscode/.local/lib/python3.12/site-packages (from docarray) (1.10.15)\n","Requirement already satisfied: rich>=13.1.0 in /home/vscode/.local/lib/python3.12/site-packages (from docarray) (13.7.1)\n","Requirement already satisfied: types-requests>=2.28.11.6 in /home/vscode/.local/lib/python3.12/site-packages (from docarray) (2.31.0.20240406)\n","Requirement already satisfied: typing-inspect>=0.8.0 in /home/vscode/.local/lib/python3.12/site-packages (from docarray) (0.9.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /home/vscode/.local/lib/python3.12/site-packages (from pydantic>=1.10.8->docarray) (4.11.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /home/vscode/.local/lib/python3.12/site-packages (from rich>=13.1.0->docarray) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vscode/.local/lib/python3.12/site-packages (from rich>=13.1.0->docarray) (2.17.2)\n","Requirement already satisfied: urllib3>=2 in /home/vscode/.local/lib/python3.12/site-packages (from types-requests>=2.28.11.6->docarray) (2.2.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /home/vscode/.local/lib/python3.12/site-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n","Requirement already satisfied: mdurl~=0.1 in /home/vscode/.local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n","Using cached docarray-0.40.0-py3-none-any.whl (270 kB)\n","Installing collected packages: docarray\n","  Attempting uninstall: docarray\n","    Found existing installation: docarray 0.38.0\n","    Uninstalling docarray-0.38.0:\n","      Successfully uninstalled docarray-0.38.0\n","Successfully installed docarray-0.40.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install --upgrade docarray"]},{"cell_type":"markdown","id":"92a9caca-70fd-4ac0-aa15-1bee55c456d3","metadata":{},"source":["### Instructions"]},{"cell_type":"markdown","id":"7f6c51c7-bbbb-4f0f-b218-850221f3dcdf","metadata":{},"source":["## Task 1: Import The Required Libraries "]},{"cell_type":"markdown","id":"2cf847fd-f8f8-49f6-9b43-0eb098239072","metadata":{},"source":["For this project we need the `os` and the `yt_dlp` packages to download the YouTube video of your choosing, convert it to an `.mp3` and save the file. We will also be using the `openai` package to make easy calls to the OpenAI models we will use. "]},{"cell_type":"markdown","id":"b1fcd794-b29c-4010-8be0-651a452b2044","metadata":{},"source":["Import the following packages.\n","\n","- Import `os` \n","- Import `openai`\n","- Import `yt_dlp` with the alias `youtube_dl`\n","- From the `yt_dlp` package, import `DowloadError`\n","- Assign `openai_api_key` to `os.getenv(\"OPENAI_API_KEY\")`"]},{"cell_type":"code","execution_count":8,"id":"a1d80acb","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting openai\n","  Using cached openai-1.23.6-py3-none-any.whl.metadata (21 kB)\n","Collecting anyio<5,>=3.5.0 (from openai)\n","  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n","Collecting distro<2,>=1.7.0 (from openai)\n","  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /home/vscode/.local/lib/python3.12/site-packages (from openai) (1.10.15)\n","Collecting sniffio (from openai)\n","  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n","Collecting tqdm>4 (from openai)\n","  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /home/vscode/.local/lib/python3.12/site-packages (from openai) (4.11.0)\n","Requirement already satisfied: idna>=2.8 in /home/vscode/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n","Requirement already satisfied: certifi in /home/vscode/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Using cached openai-1.23.6-py3-none-any.whl (311 kB)\n","Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n","Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n","Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n","Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n","Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n","Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n","Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n","Installing collected packages: tqdm, sniffio, h11, distro, httpcore, anyio, httpx, openai\n","Successfully installed anyio-4.3.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.23.6 sniffio-1.3.1 tqdm-4.66.2\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install openai"]},{"cell_type":"code","execution_count":9,"id":"541cd9f5-0aaa-4374-8411-25bedecd8c84","metadata":{"executionCancelledAt":null,"executionTime":176,"lastExecutedAt":1694705470957,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n\nimport os   #import os package \nimport glob\nimport openai #import the openai package \nimport yt_dlp as youtube_dl #import the yt_dlp package as youtube_dl\nfrom yt_dlp import DownloadError #import DownloadError from yt_dlp ","outputsMetadata":{"0":{"height":77,"type":"stream"}}},"outputs":[],"source":["# Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n","\n","# Import the os package\n","import os\n","\n","# Import the glob package\n","import glob\n","\n","# Import the openai package\n","import openai\n","\n","# Import the yt_dlp package as youtube_dl\n","import yt_dlp as youtube_dl\n","\n","# Import DownloadError from yt_dlp\n","from yt_dlp import DownloadError\n","\n","# Import DocArray\n","import docarray\n"]},{"cell_type":"markdown","id":"794e2ce2-ba13-446f-9ac7-2b5743f65a51","metadata":{},"source":["We will also assign the variable `openai_api_key` to the environment variable \"OPEN_AI_KEY\". This will help keep our key secure and remove the need to write it in the code here. "]},{"cell_type":"code","execution_count":11,"id":"b2f4cde0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting python-dotenv\n","  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.0.1\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install python-dotenv"]},{"cell_type":"code","execution_count":12,"id":"7156b205-f844-4d9e-8867-449ff5840839","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1694705474406,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"openai_api_key = os.getenv(\"OPENAI_API_KEY\")"},"outputs":[],"source":["from dotenv import load_dotenv\n","\n","# Load environment variables from .env file\n","load_dotenv()\n","\n","openai_api_key = os.getenv(\"OPENAI_API_KEY\")"]},{"cell_type":"markdown","id":"751b9539-cbf7-4d6e-9634-045345cf8a4a","metadata":{},"source":["## Task 2: Download the YouTube Video"]},{"cell_type":"markdown","id":"48abc459-48e5-4c7c-a795-daaf347ceef6","metadata":{},"source":["After creating the setup, the first step we will need to do is download the video from Youtube and convert it to an audio file (.mp3). \n","\n","We'll download a DataCamp tutorial about machine learning in Python.\n","\n","We will do this by setting a variable to store the `youtube_url` and the `output_dir` that we want the file to be stored. \n","\n","The `yt_dlp` allows us to download and convert in a few steps but does require a few configuration steps. This code is provided to you. \n","\n","Lastly, we will create a loop that looks in the `output_dir` to find any .mp3 files. Then we will store those in a list called `audio_files` that will be used later to send each file to the Whisper model for transcription. "]},{"cell_type":"markdown","id":"8f2f2698-f768-4437-8e7f-c11327d3d4a7","metadata":{},"source":["Create the following: \n","- Two variables - `youtube_url` to store the Video URL and `output_dir` that will be the directory where the audio files will be saved. \n","- For this tutorial, we can set the `youtube_url` to the following `\"https://www.youtube.com/watch?v=aqzxYofJ_ck\"`and the `output_dir`to `files/audio/`. In the future, you can change these values. \n","- Use the `ydl_config` that is provided to you "]},{"cell_type":"code","execution_count":16,"id":"ffb3836d-7b1b-47db-9ccc-6910972dd045","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":533,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1694802393469,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(f\"Downloading video from {youtube_url}\")\n\ntry:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\nexcept DownloadError:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\n\n\n","outputsMetadata":{"0":{"height":357,"type":"stream"},"1":{"height":137,"type":"stream"},"2":{"height":97,"type":"stream"},"3":{"height":37,"type":"stream"},"4":{"height":257,"type":"stream"},"5":{"height":77,"type":"stream"},"6":{"height":57,"type":"stream"},"7":{"height":57,"type":"stream"},"8":{"height":97,"type":"stream"},"9":{"height":77,"type":"stream"}}},"outputs":[],"source":["# # An example YouTube tutorial video\n","# youtube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n","# # Directory to store the downloaded video\n","# output_dir = \"files/audio/\"\n","\n","# # Config for youtube-dl\n","# ydl_config = {\n","#     \"format\": \"bestaudio/best\",\n","#     \"postprocessors\": [\n","#         {\n","#             \"key\": \"FFmpegExtractAudio\",\n","#             \"preferredcodec\": \"mp3\",\n","#             \"preferredquality\": \"192\",\n","#         }\n","#     ],\n","#     \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n","#     \"verbose\": True\n","# }\n","\n","# # Check if the output directory exists, if not create it\n","# if not os.path.exists(output_dir):\n","#     os.makedirs(output_dir)\n","\n","\n","# # Print a message indicating which video is being downloaded\n","\n","# print(f\"Downloading video from {youtube_url}\")\n","\n","\n","# # Attempt to download the video using the specified configuration\n","# # If a DownloadError occurs, attempt to download the video again\n","\n","# try:\n","#     with youtube_dl.YoutubeDL(ydl_config) as ydl:\n","#         ydl.download([youtube_url])\n","# except DownloadError:\n","#     with youtube_dl.YoutubeDL(ydl_config) as ydl:\n","#         ydl.download([youtube_url])\n"]},{"cell_type":"markdown","id":"df9c586d-309a-411b-90a5-6e81fe85eda4","metadata":{},"source":["To find the audio files that we will use the `glob`module that looks in the `output_dir` to find any .mp3 files. Then we will append the file to a list called `audio_files`. This will be used later to send each file to the Whisper model for transcription. "]},{"cell_type":"markdown","id":"0fa69a42-7065-4c3f-8699-fe48908f11b1","metadata":{},"source":["Create the following: \n","- A variable called `audio_files`that uses the glob module to find all matching files with the `.mp3` file extension \n","- Select the first first file in the list and assign it to `audio_filename`\n","- To verify the filename, print `audio_filename` "]},{"cell_type":"code","execution_count":17,"id":"c3d0a34d-ade9-4314-bc7d-480f165b3992","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1694705587367,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Find the audio file in the output directory\n\n# Define function parameters\noutput_dir = \"files/audio/\"\n\n# Find the audio file in the output directory\naudio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\naudio_filename = audio_files[0]\nprint(audio_filename)","outputsMetadata":{"0":{"height":56,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\n"]}],"source":["# Find the audio file in the output directory\n","\n","# Find all the audio files in the output directory\n","audio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\n","\n","\n","# Select the first audio file in the list\n","audio_filename = audio_files[0]\n","\n","# Print the name of the selected audio file\n","print(audio_filename)"]},{"cell_type":"markdown","id":"a9fe2a4e-b6ac-43d3-9b22-7df437015913","metadata":{},"source":["## Task 3: Transcribe the Video using Whisper"]},{"cell_type":"markdown","id":"1a00b32c-06e2-4fb1-8830-3634b13d133a","metadata":{},"source":["In this step we will take the downloaded and converted Youtube video and send it to the Whisper model to be transcribed. To do this we will create variables for the `audio_file`, for the `output_file` and the model. \n","\n","Using these variables we will:\n","- create a list to store the transcripts\n","- Read the Audio File \n","- Send the file to the Whisper Model using the OpenAI package "]},{"cell_type":"markdown","id":"e4b60c5a-ea58-469e-b699-d46ef1cc7485","metadata":{},"source":["To complete this step, create the following: \n","- A variable named `audio_file`that is assigned the `audio_filename` we created in the last step\n","- A variable named `output_file` that is assigned the value `\"files/transcripts/transcript.txt\"`\n","- A variable named `model` that is assigned the value  `\"whisper-1\"`\n","- An empty list called `transcripts`\n","- A variable named `audio` that uses the `open` method and `\"rb\"` modifier on the `audio_file`\n","- A variable to store the `response` from the `openai.Audio.transcribe` method that takes in the `model`and `audio` variables \n","- Append the `response[\"text\"]`to the `transcripts` list. "]},{"cell_type":"code","execution_count":19,"id":"54306dcc-40f7-4a12-97ef-388b95c70ad4","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":35820,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1694705690478,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Define function parameters\naudio_file = audio_filename\noutput_file = \"files/transcripts/transcript.txt\"\nmodel = \"whisper-1\"\n\n# Transcribe audio to text\nprint(audio_file)\nprint(\"converting audio to text...\")\nwith open(audio_file, \"rb\") as audio:\n    response = openai.Audio.transcribe(model, audio)\n\ntranscript = (response[\"text\"])\n\n","outputsMetadata":{"0":{"height":76,"type":"stream"}}},"outputs":[],"source":["# # Define function parameters\n","# audio_file = audio_filename\n","# output_file = \"files/transcripts/transcript.txt\"\n","# model = \"whisper-1\"\n","\n","# # Print the name of the audio file\n","# print(audio_file)\n","\n","\n","# # Transcribe the audio file to text using OpenAI API\n","# print(\"converting audio to text...\")\n","\n","# with open(audio_file, \"rb\") as audio:\n","#     response = openai.Audio.transcribe(model, audio)\n","\n","# # Extract the transcript from the response\n","# transcript = (response[\"text\"])"]},{"cell_type":"code","execution_count":22,"id":"456b2709","metadata":{},"outputs":[],"source":["from dotenv import load_dotenv\n","\n","# Load environment variables from .env file\n","load_dotenv()\n","\n","openai_api_key = os.getenv(\"OPENAI_API_KEY\")"]},{"cell_type":"code","execution_count":23,"id":"dc02f5f9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\n","converting audio to text...\n"]},{"ename":"AttributeError","evalue":"'Audio' object has no attribute 'transcribe'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverting audio to text...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(audio_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m audio:\n\u001b[0;32m---> 18\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m(model, audio)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Extract the transcript from the response\u001b[39;00m\n\u001b[1;32m     21\u001b[0m transcript \u001b[38;5;241m=\u001b[39m (response\u001b[38;5;241m.\u001b[39mtext)\n","\u001b[0;31mAttributeError\u001b[0m: 'Audio' object has no attribute 'transcribe'"]}],"source":["from openai import OpenAI\n","\n","client = OpenAI()\n","\n","# Define function parameters\n","audio_file = audio_filename\n","output_file = \"files/transcripts/transcript.txt\"\n","model = \"whisper-1\"\n","\n","# Print the name of the audio file\n","print(audio_file)\n","\n","\n","# Transcribe the audio file to text using OpenAI API\n","print(\"converting audio to text...\")\n","\n","with open(audio_file, \"rb\") as audio:\n","    response = client.audio.transcribe(model, audio)\n","\n","# Extract the transcript from the response\n","transcript = (response.text)"]},{"cell_type":"code","execution_count":25,"id":"5ebbe754","metadata":{},"outputs":[{"data":{"text/plain":["'files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3'"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["audio_filename"]},{"cell_type":"code","execution_count":24,"id":"8bc5ec81","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\n","converting audio to text...\n"]},{"ename":"RuntimeError","evalue":"Expected entry at `file` to be bytes, an io.IOBase instance, PathLike or a tuple but received <class 'str'> instead. See https://github.com/openai/openai-python/tree/main#file-uploads","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverting audio to text...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(audio_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m audio:\n\u001b[0;32m---> 19\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscriptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhisper-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m  \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Extract the transcript from the response\u001b[39;00m\n\u001b[1;32m     23\u001b[0m transcript \u001b[38;5;241m=\u001b[39m (response\u001b[38;5;241m.\u001b[39mtext)\n","File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/resources/audio/transcriptions.py:110\u001b[0m, in \u001b[0;36mTranscriptions.create\u001b[0;34m(self, file, model, language, prompt, response_format, temperature, timestamp_granularities, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mTranscribes audio into the input language.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m body \u001b[38;5;241m=\u001b[39m deepcopy_minimal(\n\u001b[1;32m    100\u001b[0m     {\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m: file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     }\n\u001b[1;32m    109\u001b[0m )\n\u001b[0;32m--> 110\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mextract_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m files:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# It should be noted that the actual Content-Type header that will be\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# sent to the server will contain a `boundary` parameter, e.g.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# multipart/form-data; boundary=---abc--\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultipart/form-data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n","File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_utils/_utils.py:52\u001b[0m, in \u001b[0;36mextract_files\u001b[0;34m(query, paths)\u001b[0m\n\u001b[1;32m     50\u001b[0m files: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, FileTypes]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[0;32m---> 52\u001b[0m     files\u001b[38;5;241m.\u001b[39mextend(\u001b[43m_extract_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflattened_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m files\n","File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_utils/_utils.py:95\u001b[0m, in \u001b[0;36m_extract_items\u001b[0;34m(obj, path, index, flattened_key)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m         flattened_key \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_extract_items\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflattened_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflattened_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list(obj):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<array>\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_utils/_utils.py:74\u001b[0m, in \u001b[0;36m_extract_items\u001b[0;34m(obj, path, index, flattened_key)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_files\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_is_file_content\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# We have exhausted the path, return the entry we found.\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[43massert_is_file_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflattened_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m flattened_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(flattened_key, cast(FileTypes, obj))]\n","File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_files.py:36\u001b[0m, in \u001b[0;36massert_is_file_content\u001b[0;34m(obj, key)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_file_content(obj):\n\u001b[1;32m     35\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected entry at `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected file input `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be bytes, an io.IOBase instance, PathLike or a tuple but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead. See https://github.com/openai/openai-python/tree/main#file-uploads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected entry at `file` to be bytes, an io.IOBase instance, PathLike or a tuple but received <class 'str'> instead. See https://github.com/openai/openai-python/tree/main#file-uploads"]}],"source":["from openai import OpenAI\n","\n","client = OpenAI()\n","\n","# Define function parameters\n","audio_file = audio_filename\n","output_file = \"files/transcripts/transcript.txt\"\n","model = \"whisper-1\"\n","\n","# Print the name of the audio file\n","print(audio_file)\n","\n","\n","# Transcribe the audio file to text using OpenAI API\n","print(\"converting audio to text...\")\n","\n","audio_file = open(audio_filename, \"rb\")\n","\n","response = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n","\n","# Extract the transcript from the response\n","transcript = (response.text)"]},{"cell_type":"markdown","id":"7255d301-69e2-4e04-813d-5a90b5ebcbdc","metadata":{},"source":["To save the transcripts to text files we will use the below provided code: "]},{"cell_type":"code","execution_count":null,"id":"6798bff4-ac8d-46e3-8c62-e540655e859d","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":27,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1694705694076,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"if output_file is not None:\n    # save transcript to a .txt file\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    with open(output_file, \"w\") as file:\n        file.write(transcript)\n\nprint(transcript)\n\n","outputsMetadata":{"0":{"height":532,"type":"stream"}}},"outputs":[],"source":["# If an output file is specified, save the transcript to a .txt file\n","\n","\n","    # Create the directory for the output file if it doesn't exist\n","\n","    # Write the transcript to the output file\n","\n","\n","# Print the transcript to the console to verify it worked \n","\n","\n"]},{"cell_type":"markdown","id":"c1001454-eb29-4981-825f-fa08e2fc48e8","metadata":{},"source":["## Task 4: Create a TextLoader using LangChain "]},{"cell_type":"markdown","id":"8191715b-72ad-4a34-ac95-02e1fbf8d391","metadata":{},"source":["In order to use text or other types of data with LangChain we must first convert that data into Documents. This is done by using loaders. In this tutorial, we will use the `TextLoader` that will take the text from our transcript and load it into a document. "]},{"cell_type":"markdown","id":"4f75f541-5bd7-4214-a75e-79681303c6f6","metadata":{},"source":["To complete this step, do the following: \n","- Import `TextLoader` from `langchain.document_loaders`\n","- Create a variable called `loader` that uses the `TextLoader` method which takes in the directory of the transcripts `\"./files/text\"`\n","- Create a variable called `docs` that is assigned the result of calling the `loader.load()` method. "]},{"cell_type":"code","execution_count":null,"id":"bb8654f7-965e-4e62-98ab-d08b7026e3d9","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1694705724878,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.document_loaders import TextLoader\n\nloader = TextLoader(\"./files/text\")\ndocs = loader.load()"},"outputs":[],"source":["# Import the TextLoader class from the langchain.document_loaders module\n","\n","\n","\n","# Create a new instance of the TextLoader class, specifying the directory containing the text files\n","\n","\n","\n","# Load the documents from the specified directory using the TextLoader instance\n","\n"]},{"cell_type":"code","execution_count":null,"id":"269aaed5-7d07-43d7-a2d0-a89730ec4bc9","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":16,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1694705727440,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"docs[0]"},"outputs":[],"source":["# Show the first element of docs to verify it has been loaded \n"]},{"cell_type":"markdown","id":"577069b3-02f6-4b73-aaaa-d8b8e8006d98","metadata":{},"source":["## Task 4: Creating an In-Memory Vector Store "]},{"cell_type":"markdown","id":"79af9e43-c32f-478d-b057-dc3b7890925e","metadata":{},"source":["Now that we have created Documents of the transcription, we will store that Document in a vector store. Vector stores allows LLMs to traverse through data to find similiarity between different data based on their distance in space. \n","\n","For large amounts of data, it is best to use a designated Vector Database. Since we are only using one transcript for this tutorial, we can create an in-memory vector store using the `docarray` package. \n","\n","We will also tokenize our queries using the `tiktoken` package. This means that our query will be seperated into smaller parts either by phrases, words or characters. Each of these parts are assigned a token which helps the model \"understand\" the text and relationships with other tokens. "]},{"cell_type":"markdown","id":"d3a5eb22-3a34-40a5-9f00-bd4895a1c4ca","metadata":{},"source":["### Instructions\n","\n","- Import the `tiktoken` package. "]},{"cell_type":"code","execution_count":null,"id":"15298bd3-5465-450d-b917-5e5d87d78bf2","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1694705815702,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import tiktoken"},"outputs":[],"source":["# Import the tiktoken package\n"]},{"cell_type":"markdown","id":"6e01af9c-f5ea-4382-b7ff-01fe0bb30edc","metadata":{},"source":["## Task 5: Create the Document Search "]},{"cell_type":"markdown","id":"22438b44-b8f8-4c78-a573-87ee4bdb2234","metadata":{},"source":["We will now use LangChain to complete some important operations to create the Question and Answer experience. Let´s import the follwing: \n","\n","- Import `RetrievalQA` from `langchain.chains` - this chain first retrieves documents from an assigned Retriver and then runs a QA chain for answering over those documents \n","- Import `ChatOpenAI` from `langchain.chat_models` - this imports the ChatOpenAI model that we will use to query the data \n","- Import `DocArrayInMemorySearch` from `langchain.vectorstores` - this gives the ability to search over the vector store we have created. \n","- Import `OpenAIEmbeddings` from `langchain.embeddings` - this will create embeddings for the data store in the vector store. \n","- Import `display` and `Markdown`from `IPython.display` - this will create formatted responses to the queries. ("]},{"cell_type":"code","execution_count":null,"id":"3a7fb40d-de20-4ec8-b05a-036b6dc6ad66","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1694706214485,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain.embeddings import OpenAIEmbeddings"},"outputs":[],"source":["# Import the RetrievalQA class from the langchain.chains module\n","\n","\n","# Import the ChatOpenAI class from the langchain.chat_models module\n","\n","\n","# Import the DocArrayInMemorySearch class from the langchain.vectorstores module\n","\n","\n","# Import the OpenAIEmbeddings class from the langchain.embeddings module\n"]},{"cell_type":"markdown","id":"9bec39d2-8a4c-4638-953f-fbd9fa47ad6f","metadata":{},"source":["Now we will create a vector store that will use the `DocArrayInMemory` search methods which will search through the created embeddings created by the OpenAI Embeddings function. "]},{"cell_type":"markdown","id":"665d55d7-25fb-4aeb-9434-6b76de0ee405","metadata":{},"source":["To complete this step: \n","- Create a variable called `db`\n","- Assign the `db` variable to store the result of the method `DocArrayInMemorySearch.from_documents`\n","- In the DocArrayInMemorySearch method, pass in the `docs` and a function call to `OpenAIEmbeddings()`"]},{"cell_type":"code","execution_count":null,"id":"66ef212c-eefd-4cf2-a02c-3c01b1b29118","metadata":{"executionCancelledAt":null,"executionTime":502,"lastExecutedAt":1694706217261,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"db = DocArrayInMemorySearch.from_documents(\n    docs, \n    OpenAIEmbeddings()\n)"},"outputs":[],"source":["# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\n"]},{"cell_type":"markdown","id":"033f3ebc-c098-49e8-a96f-f428940996d9","metadata":{},"source":["We will now create a retriever from the `db` we created in the last step. This enables the retrieval of the stored embeddings. Since we are also using the `ChatOpenAI` model, will assigned that as our LLM."]},{"cell_type":"markdown","id":"0aabff95-8fa2-47ea-b0b3-23c53c6d3c38","metadata":{},"source":["Create the following: \n","- A variable called `retriever` that is assigned `db.as_retriever()`\n","- A variable called `llm` that creates the `ChatOpenAI` method with a set `temperature`of `0.0`. This will controle the variability in the responses we receive from the LLM. "]},{"cell_type":"code","execution_count":null,"id":"7c7f6113-c145-47ff-ab9c-ada04ca047ce","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1694706219264,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"retriever = db.as_retriever() \nllm = ChatOpenAI(temperature = 0.0)"},"outputs":[],"source":["# Convert the DocArrayInMemorySearch instance to a retriever\n","\n","\n","# Create a new ChatOpenAI instance with a temperature of 0.0\n"]},{"cell_type":"markdown","id":"4f5a7c7a-1676-41e0-976a-50316a684d12","metadata":{},"source":["Our last step before starting to ask questions is to create the `RetrievalQA` chain. This chain takes in the:  \n","- The `llm` we want to use\n","- The `chain_type` which is how the model retrieves the data\n","- The `retriever` that we have created \n","- An option called `verbose` that allows use to see the seperate steps of the chain "]},{"cell_type":"markdown","id":"2a5ce4f9-e025-40e6-b737-be77213d5110","metadata":{},"source":["Create a variable called `qa_stuff`. This variable will be assigned the method `RetrievalQA.from_chain_type`. \n","\n","Use the following settings inside this method: \n","- `llm=llm`\n","- `chain_type=\"stuff\"`\n","- `retriever=retriever`\n","- `verbose=True`"]},{"cell_type":"code","execution_count":null,"id":"09fc202b-198f-4510-8d81-258f914d5c08","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1694706178555,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"qa_stuff = RetrievalQA.from_chain_type(\n\nllm=llm,\n\nchain_type=\"stuff\",\n\nretriever=retriever,\n\nverbose=True\n\n)"},"outputs":[],"source":["# Create a new RetrievalQA instance with the specified parameters\n","\n","    # The ChatOpenAI instance to use for generating responses\n","    # The type of chain to use for the QA system\n","    # The retriever to use for retrieving relevant documents\n","    # Whether to print verbose output during retrieval and generation\n"]},{"cell_type":"markdown","id":"4ce8cff5-ab49-44f0-96ee-88826d88ea6a","metadata":{},"source":["## Task 5: Create the Queries "]},{"cell_type":"markdown","id":"d51218d4-4e81-4d87-9f3f-77eacde057c1","metadata":{},"source":["Now we are ready to create queries about the YouTube video and read the responses from the LLM. This done first by creating a query and then running the RetrievalQA we setup in the last step and passing it the query. "]},{"cell_type":"markdown","id":"5e2b036b-cef6-4b52-9421-5ccf2b865482","metadata":{},"source":["To create the questions to ask the model complete the following steps: \n","- Create a variable call `query` and assigned it a string value of `\"What is this tutorial about?\"`\n","- Create a `response` variable that will store the result of `qa_stuff.run(query)` \n","- Show the `resposnse`"]},{"cell_type":"code","execution_count":null,"id":"d576672c-5078-487a-9dc5-3703f17d82f1","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"}}},"outputs":[],"source":["# Set the query to be used for the QA system\n","\n","\n","# Run the query through the RetrievalQA instance and store the response\n","\n","\n","# Print the response to the console\n","\n"]},{"cell_type":"markdown","id":"de9f2df3-87d1-40d3-862a-95769c11d015","metadata":{},"source":["We can continue on creating queries and even creating queries that we know would not be answered in this video to see how the model responds. "]},{"cell_type":"code","execution_count":null,"id":"dbb75225-76c1-4eb8-9055-e13a3bb68682","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"}}},"outputs":[],"source":["# Set the query to be used for the QA system\n","query = \"What is the difference between a training set and test set?\"\n","\n","# Run the query through the RetrievalQA instance and store the response\n","\n","\n","# Print the response to the console\n"]},{"cell_type":"code","execution_count":null,"id":"13864a14-0eda-4afd-bfe5-90fdefbc5d49","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"}}},"outputs":[],"source":["# Set the query to be used for the QA system\n","query = \"Who should watch this lesson?\"\n","\n","# Run the query through the RetrievalQA instance and store the response\n","\n","\n","# Print the response to the console\n"]},{"cell_type":"code","execution_count":null,"id":"c62b73e4-e746-49f9-8106-921cbb4e6df8","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"}}},"outputs":[],"source":["# Set the query to be used for the QA system\n","query =\"Who is the greatest football team on earth?\"\n","\n","# Run the query through the RetrievalQA instance and store the response\n","\n","\n","# Print the response to the console\n"]},{"cell_type":"code","execution_count":null,"id":"f9f7a7f3-f0f1-44ad-be76-d15aa009ac34","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"},"1":{"height":77,"type":"stream"}}},"outputs":[],"source":["# Set the query to be used for the QA system\n","query = \"How long is the circumference of the earth?\"\n","\n","# Run the query through the RetrievalQA instance and store the response\n","\n","\n","# Print the response to the console\n"]},{"cell_type":"markdown","id":"65454beb-970f-4af6-a04c-798b9f665b6f","metadata":{},"source":["## All done, congrats! "]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":5}
